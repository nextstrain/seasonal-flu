# Custom settings for running builds on a SLURM cluster.

# Submit a reasonable number of simultaneous jobs, accounting for the number of
# CPUs alloted to our lab's SLURM account.
cores: 20

# Always let the user know which commands are being executed for each rule and
# why each rule is being run.
printshellcmds: True
reason: True

# Track the run times of each job for debugging.
stats: stats.json

# Allow jobs to restart multiple times, since most jobs will run on the
# "restart" partition of the Hutch's cluster and can thus be killed at any time
# to make room for higher priority jobs.
restart-times: 3

# Wait a fixed number of seconds for missing files since the cluster file system
# can be quite slow and the workflow can fail unnecessarily due to this latency.
latency-wait: 60

# Use conda environments for augur and related binaries. This is important in a
# cluster environment where Docker is not available and Singularity may be
# prohibitively complicated to setup.
use-conda: True
conda-frontend: mamba

# Define cluster-specific settings for resources required by any rule. An
# important resource for the Hutch cluster is the requested "partition". Jobs
# submitted to the "restart" partition will start running almost immediately,
# but they may also be killed at any moment when someone else needs those
# resources. This is analogous to the spot resources on AWS.
default-resources:
  - mem_mb=256
  - runtime="0:05:00"
  - partition="campus-new"
  - qos="campus-new"

# Submit jobs to the cluster with the SLURM sbatch command. The string
# associated with this key tells Snakemake how to connect cluster resources with
# those defined in the default resources below.
cluster:
  mkdir -p slurm_logs/{rule} &&
  sbatch
    --parsable
    -p {resources.partition}
    --qos={resources.qos}
    --nodes=1
    --ntasks=1
    --mem={resources.mem_mb}
    --cpus-per-task={threads}
    --time={resources.runtime}
    --output=slurm_logs/{rule}/{rule}-{wildcards}.out
    --error=slurm_logs/{rule}/{rule}-{wildcards}.err
    --job-name={rule}-{wildcards}

# Tell Snakemake how to cancel jobs on the cluster when user uses Ctrl-C.
cluster-cancel: scancel

# Don't check for cluster status too frequently.
max-status-checks-per-second: 1

# Don't swamp the cluster's scheduler.
max-jobs-per-second: 10

# Limit local cores to avoid slamming the head node.
local-cores: 1

# Don't try to optimize scheduling based on available resources; the cluster can
# handle that for us.
scheduler: greedy

# Print logs for failed jobs to stdout/stderr for easier debugging.
show-failed-logs: true
