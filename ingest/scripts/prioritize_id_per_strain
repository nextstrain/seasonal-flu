#!/usr/bin/env python3
"""
Determine which record to prioritize for duplicate strain names.
Outputs TSV is expected to be used by downstream scripts/dedup-by-strain to
keep the prioritized record per strain.
"""
import argparse
import csv
from enum import Enum
from sys import stdin
from typing import Iterable
from augur.io.json import load_ndjson


class Prioritization(Enum):
    """
    Order of prioritization:
    1. Strain id is included in the TSV providied via `--prioritized-ids`
    2. Record has HA segment
    3. Record has NA segment
    4. If all else equal, first record wins.
    """
    HARDCODED = "hardcoded"
    HAS_HA = "has HA segment"
    HAS_NA = "has NA segment"
    FIRST_RECORD = "first record"


def load_prioritized_ids(prioritized_ids_file: str) -> dict:
    """
    Parse *prioritized_ids_file* to return a dict with the
    `strain` as the key and `id` as the value.

    Raises an Exception if there are duplicate strains in the file.
    """
    prioritized_ids = {}
    with open(prioritized_ids_file, "r", newline="") as fh:
        reader = csv.DictReader(fh, delimiter="\t")
        for row in reader:
            strain = row["strain"]
            record_id = row["id"]

            if strain in prioritized_ids:
                raise Exception(f"Found duplicate strain {strain!r} in {prioritized_ids_file!r}.")

            prioritized_ids[strain] = record_id

    return prioritized_ids


def determine_prioritized_ids(records: Iterable[dict],
                              strain_field: str,
                              id_field: str,
                              seq_field: str,
                              hardcoded_prioritized_ids: dict) -> dict:
    """
    Determine the prioritized id per strain.
    """
    prioritized_ids = {
        strain: {
            "id": id,
            "reasons": {Prioritization.HARDCODED}
        } for strain, id in hardcoded_prioritized_ids.items()}

    for record in records:
        strain = record.get(strain_field)
        record_id = record.get(id_field)
        sequences = record.get(seq_field)

        if strain is None:
            raise Exception(f"Records must have the expected strain field {strain_field!r}")

        if record_id is None:
            raise Exception(f"Records must have the expected id field {id_field!r}")

        if sequences is None:
            raise Exception(f"Records must have the expected sequences field {seq_field!r}")

        # Compare against already prioritized record
        if prioritized := prioritized_ids.get(strain):
            new_reasons = get_reasons(sequences, first_record = False)
            if use_new_record(prioritized["reasons"], new_reasons):
                prioritized_ids[strain] = {
                    "id": record_id,
                    "reasons": new_reasons
                }
        else:
            prioritized_ids[strain] = {
                "id": record_id,
                "reasons": get_reasons(sequences, first_record = True)
            }

    return prioritized_ids


def get_reasons(sequences: dict, first_record: bool) -> set:
    """
    Parse record *sequences* to create of set of `Prioritization` reasons.
    """
    reasons = set()

    if first_record:
        reasons.add(Prioritization.FIRST_RECORD)

    if len(sequences.get("ha", [])) > 0:
        reasons.add(Prioritization.HAS_HA)

    if len(sequences.get("na", [])) > 0:
        reasons.add(Prioritization.HAS_NA)

    return reasons


def use_new_record(old_reasons: set, new_reasons: set) -> bool:
    """
    Compare provided *old_reasons* and *new_reasons*.
    Returns True if *new_reasons* has higher priority
    """
    # Always prioritize old record if it's hardcoded
    if Prioritization.HARDCODED in old_reasons:
        return False

    # New record has HA but the old one does not
    if Prioritization.HAS_HA in new_reasons and Prioritization.HAS_HA not in old_reasons:
        return True

    # New record has NA but the old one does not
    if Prioritization.HAS_NA in new_reasons and Prioritization.HAS_NA not in old_reasons:
        return True

    # If everything is equal, keep the old record
    return False


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description=__doc__)

    parser.add_argument("--strain-field", default="strain",
        help="The record field containing the strain name.")
    parser.add_argument("--id-field", default="gisaid_epi_isl",
        help="The record field containing a record id. " + \
             "If providing the `--prioritized-ids` file, then this id field " + \
             "will be used to match the prioritized ids. " + \
             "Also included in the warning logs for records with duplicate strains.")
    parser.add_argument("--seq-field", default="sequences",
        help="The record field containing the sequences." + \
             "Allows the script to prioritize the records with HA and NA segments." )
    parser.add_argument("--prioritized-ids",
        help="An optional TSV file with prioritized ids for specific strains. " + \
             "The file is expected to have the columns `strain` and `id` " + \
             "and the `strain` column is expected to have unique strains. " + \
             "Any records with a strain that is included in this file " + \
             "will be dropped if the record id does not match the prioritized id. ")
    parser.add_argument("--output", required=True,
        help="Output of prioritized ids per strain.")

    args = parser.parse_args()

    hardcoded_prioritized_ids = {}
    if args.prioritized_ids:
        hardcoded_prioritized_ids = load_prioritized_ids(args.prioritized_ids)

    records = load_ndjson(stdin)
    prioritized_ids_per_strain = determine_prioritized_ids(
        records,
        args.strain_field,
        args.id_field,
        args.seq_field,
        hardcoded_prioritized_ids)

    with open(args.output, 'w', newline='') as output_handle:
        tsv_writer = csv.writer(output_handle, delimiter='\t')
        tsv_writer.writerow(["strain", "id", "reasons"])
        for strain, prioritized in prioritized_ids_per_strain.items():
            sorted_reasons = sorted(prioritized["reasons"], key=lambda x: list(Prioritization).index(x))
            tsv_writer.writerow([strain, prioritized["id"], ",".join(reason.value for reason in sorted_reasons)])
